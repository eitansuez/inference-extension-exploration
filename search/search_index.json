{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This site contains two exercises.</p> <p>The first is my personal clarification of the steps from the guide published on inference extension to the gw api.  This is an exercise in deploying the extension with Envoy Gateway.</p> <p>The second is a repetition of the exercise using kgateway.</p>"},{"location":"eg/","title":"Understanding the Inference Extension Guide","text":"<p>The GW API inference extension has a guide demonstrating how the extension works.</p> <p>The idea is to setup in a Kubernetes cluster a mechanism to route traffic to backend LLMs.</p>"},{"location":"eg/#prerequisites","title":"Prerequisites","text":"<ul> <li>amd64 CPU architecture (required by vLLM).</li> <li>Lots of CPU and RAM.</li> <li>Tools:  kubectl, helm</li> </ul>"},{"location":"eg/#a-k8s-cluster","title":"A k8s cluster","text":"<p>I chose k3d:</p> <pre><code>k3d cluster create my-k8s-cluster \\\n    --api-port 6443 \\\n    --k3s-arg \"--disable=traefik@server:0\" \\\n    --port 80:80@loadbalancer \\\n    --port 443:443@loadbalancer\n</code></pre>"},{"location":"eg/#envoy-gateway","title":"Envoy Gateway","text":"<p>The guide uses Envoy Gateway.</p> <pre><code>helm install eg oci://docker.io/envoyproxy/gateway-helm \\\n  --version v1.3.1 \\\n  -n envoy-gateway-system --create-namespace\n</code></pre> <p>It's interesting to point out that Envoy Gateway will automatically install the Kubernetes GW API CRDs as part of its installation.  So no need to do that explicitly.</p> <pre><code>k api-resources --api-group=gateway.networking.k8s.io\n</code></pre>"},{"location":"eg/#enable-patch-policy","title":"Enable Patch Policy","text":"<p>This step is mostly artificial, or an artifact of Envoy Gateway conventions:  by default Envoy Gateway does not permit the application of policies that configure a gateway directly.</p> <p>All we are doing is telling Envoy Gateway through configuration:  enable it.</p> eg/enable-patch-policy.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: envoy-gateway-config\n  namespace: envoy-gateway-system\ndata:\n# This manifest's main purpose is to set `enabledEnvoyPatchPolicy` to `true`.\n# This only needs to be ran once on your cluster (unless you'd like to change anything. i.e. enabling the admin dash)\n# Any field under `admin` is optional, and only for enabling the admin endpoints, for debugging.\n# Admin Interface: https://www.envoyproxy.io/docs/envoy/latest/operations/admin\n# PatchPolicy docs: https://gateway.envoyproxy.io/docs/tasks/extensibility/envoy-patch-policy/#enable-envoypatchpolicy \n  envoy-gateway.yaml: |\n    apiVersion: gateway.envoyproxy.io/v1alpha1\n    kind: EnvoyGateway\n    provider:\n      type: Kubernetes\n    gateway:\n      controllerName: gateway.envoyproxy.io/gatewayclass-controller\n    extensionApis:\n      enableEnvoyPatchPolicy: true      \n      enableBackend: true\n#    admin:\n#      enablePprof: true\n#      address:\n#        host: 127.0.0.1\n#        port: 19000\n#      enabledDumpConfig: true\n</code></pre> <pre><code>k apply -f eg/enable-patch-policy.yaml\n</code></pre> <p>After the ConfigMap is revised, the deployment must be restarted to pick up the change:</p> <pre><code>k rollout restart deployment envoy-gateway -n envoy-gateway-system\n</code></pre>"},{"location":"eg/#gateway-class","title":"Gateway Class","text":"<p>The installation of Envoy Gateway does not automatically beget a GatewayClass.</p> <p>So let's go ahead and create it:</p> eg/gateway-class.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: eg\nspec:\n  controllerName: gateway.envoyproxy.io/gatewayclass-controller\n</code></pre> <pre><code>k apply -f eg/gateway-class.yaml\n</code></pre>"},{"location":"eg/#provision-a-gateway","title":"Provision a Gateway","text":"eg/gateway.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: inference-gateway\nspec:\n  gatewayClassName: eg\n  listeners:\n    - name: http\n      protocol: HTTP\n      port: 8080\n    - name: llm-gw\n      protocol: HTTP\n      port: 8081\n</code></pre> <p>The configuration is straightforward: the gatewayClassName points to the eg gateway class.</p> <p>It's curious that two listeners are configured, presumably to demonstrate that you can segregate LLM requests from non-LLM requests.  In other words, dedicate a listener for LLM requests named <code>llm-gw</code>.</p> <pre><code>k apply -f eg/gateway.yaml\n</code></pre>"},{"location":"eg/#deploy-an-llm-worload","title":"Deploy an LLM worload","text":"<p>The guide offers two options.  We opt for the cpu deployment option.</p> eg/cpu-deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama2-7b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-llama2-7b\n  template:\n    metadata:\n      labels:\n        app: vllm-llama2-7b\n    spec:\n      containers:\n        - name: lora\n          image: \"public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.0\" # formal images can be found in https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n          - \"--model\"\n          - \"Qwen/Qwen2.5-1.5B-Instruct\"\n          - \"--port\"\n          - \"8000\"\n          - \"--enable-lora\"\n          - \"--max-loras\"\n          - \"4\"\n          - \"--lora-modules\"\n          - '{\"name\": \"tweet-summary-0\", \"path\": \"SriSanth2345/Qwen-1.5B-Tweet-Generations\", \"base_model_name\": \"Qwen/Qwen2.5-1.5B\"}'\n          - '{\"name\": \"tweet-summary-1\", \"path\": \"SriSanth2345/Qwen-1.5B-Tweet-Generations\", \"base_model_name\": \"Qwen/Qwen2.5-1.5B\"}'\n          env:\n            - name: PORT\n              value: \"8000\"\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n            - name: VLLM_CPU_KVCACHE_SPACE\n              value: \"4\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 240\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 600\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          resources:\n             limits:\n               cpu: \"12\"\n               memory: \"9000Mi\"\n             requests:\n               cpu: \"12\"\n               memory: \"9000Mi\"\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts: # DO NOT USE subPath, dynamic configmap updates don't work on subPaths\n          - name: config-volume\n            mountPath:  /config\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-qwen-adapters\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-qwen-adapters\ndata:\n  configmap.yaml: |\n      vLLMLoRAConfig:\n        name: vllm-llama2-7b\n        port: 8000\n        ensureExist:\n          models:\n          - base-model: Qwen/Qwen2.5-1.5B\n            id: tweet-summary-1\n            source: SriSanth2345/Qwen-1.5B-Tweet-Generations\n</code></pre> <p>The cpu deployment option uses a relatively small model:  <code>Qwen/Qwen2.5-1.5B-Instruct</code>. This model is not gated and so does not require a Hugging Face token.</p> <p>The container requires 12 cpus and 9Gi of memory (see the <code>resources</code> section).</p> <p>The perhaps important thing to note about the running container is that it supposedly configures two models, <code>tweet-summary-0</code> and <code>tweet-summary-1</code> through lora adapters that extend the base model.</p> <p>The default deployment sets 3 replicas which requires a lot of resources. So I set my manifest to use just one replica.</p> <pre><code>k apply -f eg/cpu-deployment.yaml\n</code></pre> <p>Allow 5-10 minutes before the pod is up and running.</p>"},{"location":"eg/#inference-extension","title":"Inference Extension","text":""},{"location":"eg/#install-the-crds","title":"Install the CRDs","text":"<p>The inference extension provides two new resources: InferencePool and InferenceModel for configuring different facets of routing to LLM backends.</p> <p>The cluster needs to know about these resources:</p> <pre><code>k apply -f crds/\n</code></pre>"},{"location":"eg/#install-the-extension","title":"Install the extension","text":"eg/inference-extension.yaml<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-llama2-7b-epp\n  namespace: default\nspec:\n  selector:\n    app: vllm-llama2-7b-epp\n  ports:\n    - protocol: TCP\n      port: 9002\n      targetPort: 9002\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama2-7b-epp\n  namespace: default\n  labels:\n    app: vllm-llama2-7b-epp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-llama2-7b-epp\n  template:\n    metadata:\n      labels:\n        app: vllm-llama2-7b-epp\n    spec:\n      containers:\n      - name: epp\n        image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:main\n        imagePullPolicy: Always\n        args:\n        - -poolName\n        - \"vllm-llama2-7b\"\n        - -v\n        - \"4\"\n        - -grpcPort\n        - \"9002\"\n        - -grpcHealthPort\n        - \"9003\"\n        env:\n        - name: USE_STREAMING\n          value: \"true\"\n        ports:\n        - containerPort: 9002\n        - containerPort: 9003\n        - name: metrics\n          containerPort: 9090\n        livenessProbe:\n          grpc:\n            port: 9003\n            service: inference-extension\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          grpc:\n            port: 9003\n            service: inference-extension\n          initialDelaySeconds: 5\n          periodSeconds: 10\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: pod-read\nrules:\n- apiGroups: [\"inference.networking.x-k8s.io\"]\n  resources: [\"inferencemodels\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"inference.networking.x-k8s.io\"]\n  resources: [\"inferencepools\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"discovery.k8s.io\"]\n  resources: [\"endpointslices\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n--- \nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: pod-read-binding\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: pod-read\n</code></pre> <pre><code>k apply -f eg/inference-extension.yaml\n</code></pre>"},{"location":"eg/#hook-in-the-extension","title":"Hook in the extension","text":"<p>To hook the extension into the flow of requests through the gateway's <code>llm-gw</code> listener on port 8081, we define a ficticious backend and a route to it:</p> eg/backend-and-route.yaml<pre><code>---\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: Backend\nmetadata:\n  name: backend-dummy\nspec:\n  endpoints:\n    - fqdn:\n        # Both these values are arbitrary and unused as the PatchPolicy redirects requests.\n        hostname: 'foo.bar.com'\n        port: 8080\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - name: inference-gateway\n    sectionName: llm-gw\n  rules:\n  - backendRefs:\n    - group: gateway.envoyproxy.io\n      kind: Backend\n      name: backend-dummy\n    timeouts:\n      request: \"24h\"\n      backendRequest: \"24h\"\n</code></pre> <p>Note that the route has no match clause, meaning any request to that listener matches.</p> <pre><code>k apply -f eg/backend-and-route.yaml\n</code></pre> <p>Next, we define an EG extension policy:</p> eg/extension-policy.yaml<pre><code>---\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyExtensionPolicy\nmetadata:\n  name: ext-proc-policy\n  namespace: default\nspec:\n  extProc:\n    - backendRefs:\n      - group: \"\"\n        kind: Service\n        name: vllm-llama2-7b-epp\n        port: 9002\n      processingMode:\n        allowModeOverride: true\n        request:\n          body: Buffered\n        response:\n      # The timeouts are likely not needed here. We can experiment with removing/tuning them slowly.\n      # The connection limits are more important and will cause the opaque: ext_proc_gRPC_error_14 error in Envoy GW if not configured correctly. \n      messageTimeout: 1000s\n      backendSettings:\n        circuitBreaker:\n          maxConnections: 40000\n          maxPendingRequests: 40000\n          maxParallelRequests: 40000\n        timeout:\n          tcp:\n            connectTimeout: 24h\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: llm-route\n</code></pre> <p>The extension policy achieves two things:</p> <ol> <li>It makes EG aware of the inference extension you just deployed through the <code>extProc</code> section, and</li> <li>It attaches the extension to the above route</li> </ol> <pre><code>k apply -f eg/extension-policy.yaml\n</code></pre>"},{"location":"eg/#envoy-patch-policy","title":"Envoy Patch Policy","text":"<p>The guide also applies an Envoy patch policy.</p> eg/patch-policy.yaml<pre><code>---\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyPatchPolicy\nmetadata:\n  name: custom-response-patch-policy\n  namespace: default\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway\n    name: inference-gateway\n  type: JSONPatch\n  jsonPatches:\n    # Necessary to create a cluster of the type: ORIGINAL_DST to allow for \n    # direct pod scheduling. Which is heavily utilized in our scheduling.\n    # Specifically the field `original_dst_lb_config` allows us to enable\n    # `use_http_header` and `http_header_name`. \n    # Source: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/cluster/v3/cluster.proto\n    - type: \"type.googleapis.com/envoy.config.cluster.v3.Cluster\"\n      name: original_destination_cluster\n      operation:\n        op: add\n        path: \"\"\n        value:\n          name: original_destination_cluster\n          type: ORIGINAL_DST\n          original_dst_lb_config:\n            use_http_header: true\n            http_header_name: \"x-gateway-destination-endpoint\"\n          connect_timeout: 1000s\n          lb_policy: CLUSTER_PROVIDED\n          dns_lookup_family: V4_ONLY\n          circuit_breakers:\n            thresholds:\n            - max_connections: 40000\n              max_pending_requests: 40000\n              max_requests: 40000\n\n    # This ensures that envoy accepts untrusted certificates. We tried to explicitly\n    # set TrustChainVerification to ACCEPT_UNSTRUSTED, but that actually didn't work\n    # and what worked is setting the common_tls_context to empty.\n    - type: \"type.googleapis.com/envoy.config.cluster.v3.Cluster\"\n      name: \"envoyextensionpolicy/default/ext-proc-policy/extproc/0\"\n      operation:\n        op: add\n        path: \"/transport_socket\"\n        value:\n          name: \"envoy.transport_sockets.tls\"\n          typed_config:\n            \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\"\n            common_tls_context: {}\n    - type: \"type.googleapis.com/envoy.config.route.v3.RouteConfiguration\"\n      name: default/inference-gateway/llm-gw\n      operation:\n        op: replace\n        path: \"/virtual_hosts/0/routes/0/route/cluster\"\n        value: original_destination_cluster\n# Comment the below to disable full duplex streaming\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: add\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/request_body_mode\"\n        value: FULL_DUPLEX_STREAMED\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: add\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/request_trailer_mode\"\n        value: SEND\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: add\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/response_body_mode\"\n        value: FULL_DUPLEX_STREAMED\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: replace\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/response_trailer_mode\"\n        value: SEND\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: replace\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/response_header_mode\"\n        value: SEND\n</code></pre> <p>I don't really understand what this policy does or its purpose, if it circumvents a bug, or what.</p> <pre><code>k apply -f eg/patch-policy.yaml\n</code></pre>"},{"location":"eg/#configure-the-extension","title":"Configure the extension","text":"<p>Now that the extension is in the path of requests, we need to give it the information it needs to do its job:  it needs to know about the backend LLMs, associate model names to each, and other metadata such as \"criticality\" in order to know the relative importance of the different models.</p>"},{"location":"eg/#the-inferencepool","title":"The InferencePool","text":"eg/inference-pool.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: vllm-llama2-7b\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama2-7b\n  extensionRef:\n    name: vllm-llama2-7b-epp\n</code></pre> <p>The name of the pool is set to <code>vllm-llama2-7b</code>.</p> <p>The InferencePool resource appears to be concerned primarily with more practical aspects of how to route to the LLM workload:</p> <ol> <li>What port does it run (<code>targetPortNumber</code>), and </li> <li>How do I select or identify the workload (<code>selector</code>)</li> </ol> <pre><code>k apply -f eg/inference-pool.yaml\n</code></pre>"},{"location":"eg/#the-inferencemodel","title":"The InferenceModel","text":"eg/inference-model.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: inferencemodel-sample\nspec:\n  modelName: tweet-summary\n  criticality: Critical\n  poolRef:\n    name: vllm-llama2-7b\n  targetModels:\n  - name: tweet-summary-1\n    weight: 100\n</code></pre> <p>The model achieves the following:</p> <ol> <li>Any requests specifying the model \"tweet-summary\" will match this inference model</li> <li>This model is marked with a <code>criticality</code> of Critical</li> <li>This model is associated with the above inference pool, meaning that matching requests will be routed to our cpu deployment</li> <li>Through <code>targetModels</code> we are configuring which of the two extension models (\"tweet-summary-0\" or \"tweet-summary-1\") to target, or what weight distributions to give to each.</li> </ol> <pre><code>k apply -f eg/inference-model.yaml\n</code></pre>"},{"location":"eg/#test-it","title":"Test it","text":"<p>Everything is now in place:  requests can be sent in to the gateway, the inference extension will be consulted, it will inspect the request, note the model <code>tweet-summary</code> in the request, which will match the above InferenceModel, resulting in the request being routed to the <code>tweet-summary-1</code> model running as part of the cpu deployment we deployed earlier.</p> <p>I suspect that the name \"llama2-7b\" is an artifact of perhaps a prior attempt to make this exercise work with a different model.  The model was revised to Qwen-1.5B but the name of the deployment and associated labels were never updated to match?</p> <p>Capture the gateway IP address to the environment variable <code>GW_IP</code>:</p> <pre><code>export GW_IP=$(kubectl get gtw inference-gateway -o jsonpath='{.status.addresses[0].value}')\n</code></pre> <p>Send a request:</p> <pre><code>curl $GW_IP:8081/v1/completions -H 'Content-Type: application/json' -d '{\n  \"model\": \"tweet-summary\",\n  \"prompt\": \"Write as if you were a critic: San Francisco\",\n  \"max_tokens\": 100,\n  \"temperature\": 0\n}' | jq\n</code></pre> <p>Here is a sample response:</p> <pre><code>{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"prompt_logprobs\": null,\n      \"stop_reason\": null,\n      \"text\": \" Giants - 2019\\n\\nThe San Francisco Giants have been one of the most successful teams in Major League Baseball over the past few years, and they continue to be a force to be reckoned with. The team has won three World Series championships in the last five seasons, including their first title since 1954.\\n\\nIn 2019, the Giants continued their dominance by winning the National League West divisional title for the third time in four years. They finished the\"\n    }\n  ],\n  \"created\": 1742577733,\n  \"id\": \"cmpl-30ea8f02-2aa7-4648-bb28-efe88c43517c\",\n  \"model\": \"tweet-summary-1\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 100,\n    \"prompt_tokens\": 10,\n    \"prompt_tokens_details\": null,\n    \"total_tokens\": 110\n  }\n}\n</code></pre> <p>Consider playing with the InferenceModel:</p> <ul> <li>Target the other model instead.</li> <li>Make another curl request</li> <li>Verify in the response that the request was handled by that model</li> </ul>"},{"location":"eg/#thoughts","title":"Thoughts","text":"<p>As I review this model diagram:</p> <p></p> <p>The configuration doesn't match this picture perfectly.</p> <p>Clearly the route and inference pool are associated to one another through the extension, which is not depicted above.</p> <p>In my opinion the extension ought to be transparent, and the inference resources attached directly to the route.</p> <p>Also the dichotomy of InferencePool and InferenceModel don't make very much sense to me, yet, perhaps due to lack of familiarity with this project.</p>"},{"location":"kgateway-with-fake-llm/","title":"Inference extension with kgateway, using a fake LLM","text":"<p>This guide is similar to the lab that uses kgateway. The difference is its use of a fake for the LLM deployment, so you can experiment with the inference extension without the cpu and memory resources needed to actually run an LLM.</p> <p>Provision a Kubernetes cluster:</p> <pre><code>k3d cluster create my-k8s-cluster \\\n    --api-port 6443 \\\n    --k3s-arg \"--disable=traefik@server:0\" \\\n    --port 80:80@loadbalancer \\\n    --port 443:443@loadbalancer\n</code></pre>"},{"location":"kgateway-with-fake-llm/#install-kgateway","title":"Install kgateway","text":"<p>Install the GW API CRDs:</p> <pre><code>kubectl apply -f \\\n  https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.1/experimental-install.yaml\n</code></pre> <p>The inference extension CRDs also need to be installed:</p> <pre><code>k apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.2.0/manifests.yaml\n</code></pre> <p>Install the kgateway CRDs:</p> <pre><code>helm upgrade --install kgateway-crds \\\n  oci://cr.kgateway.dev/kgateway-dev/charts/kgateway-crds \\\n  --version v2.0.0-main \\\n  --namespace kgateway-system --create-namespace\n</code></pre> <p>Install kgateway:</p> <pre><code>helm upgrade --install kgateway \\\n  oci://cr.kgateway.dev/kgateway-dev/charts/kgateway \\\n  --version v2.0.0-main \\\n  --namespace kgateway-system --create-namespace \\\n  --set inferenceExtension.enabled=true\n</code></pre> <p>Above, note that the inference extension is enabled.</p>"},{"location":"kgateway-with-fake-llm/#deploy-a-fake-llm-worload","title":"Deploy a fake LLM worload","text":"fakellm/fake-llm.yaml<pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fake-llm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fake-llm\n  template:\n    metadata:\n      labels:\n        app: fake-llm\n    spec:\n      terminationGracePeriodSeconds: 5  # Shorten to 5 seconds\n      containers:\n      - name: fake-llm\n        image: eitansuez/fake-llm:2.4\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            cpu: \"0.5\"\n            memory: \"512Mi\"\n          requests:\n            cpu: \"0.2\"\n            memory: \"256Mi\"\n</code></pre> <p>This fake llm is configured with the model name <code>fake-llm</code> and two alternative LoRA modules:  <code>fake-llm-lora-a</code> and <code>fake-llm-lora-b</code>.</p> <pre><code>k apply -f fakellm/fake-llm.yaml\n</code></pre>"},{"location":"kgateway-with-fake-llm/#the-inferencepool","title":"The InferencePool","text":"fakellm/inferencepool.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: fakellm-pool\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: fake-llm\n  extensionRef:\n    name: fakellm-pool-endpoint-picker\n</code></pre> <p>The name of the pool is set to <code>fakellm-pool</code>.</p> <p>The InferencePool resource appears to be concerned primarily with more practical aspects of how to route to the LLM workload:</p> <ol> <li>What port does it run (<code>targetPortNumber</code>), and </li> <li>How do I select or identify the workload (<code>selector</code>)</li> </ol> <pre><code>k apply -f fakellm/inferencepool.yaml\n</code></pre>"},{"location":"kgateway-with-fake-llm/#the-inferencemodel","title":"The InferenceModel","text":"fakellm/inferencemodel.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: fakellm-inferencemodel\nspec:\n  modelName: fake-llm\n  criticality: Critical\n  poolRef:\n    name: fakellm-pool\n  targetModels:\n  - name: fake-llm-lora-a\n    weight: 100\n</code></pre> <p>The model achieves the following:</p> <ol> <li>Any requests specifying the model <code>fake-llm</code> will match this inference model</li> <li>This model is marked with a <code>criticality</code> of <code>Critical</code></li> <li>This model is associated with the above inference pool, meaning that matching requests will be routed to the fake llm deployment</li> <li>Through <code>targetModels</code> we are configuring which of the two extension models (<code>fake-llm-lora-a</code> or <code>fake-llm-lora-b</code>) to target, or what weight distributions to give to each.</li> </ol> <pre><code>k apply -f fakellm/inferencemodel.yaml\n</code></pre>"},{"location":"kgateway-with-fake-llm/#provision-a-gateway","title":"Provision a Gateway","text":"fakellm/gateway.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: inference-gateway\nspec:\n  gatewayClassName: kgateway\n  listeners:\n  - name: llm-gw\n    protocol: HTTP\n    port: 80\n</code></pre> <p>The configuration is straightforward: the <code>gatewayClassName</code> is set to <code>kgateway</code>.</p> <pre><code>k apply -f fakellm/gateway.yaml\n</code></pre>"},{"location":"kgateway-with-fake-llm/#configure-the-route","title":"Configure the route","text":"fakellm/route.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - name: inference-gateway\n    sectionName: llm-gw\n  rules:\n  - backendRefs:\n    - group: inference.networking.x-k8s.io\n      kind: InferencePool\n      name: fakellm-pool\n      port: 8000\n</code></pre> <p>The route forwards requests to the inference pool directly by referencing it as a <code>backendRef</code>.</p> <pre><code>k apply -f fakellm/route.yaml\n</code></pre>"},{"location":"kgateway-with-fake-llm/#test-it","title":"Test it","text":"<p>Everything is now in place:  requests can be sent in to the gateway, the inference extension will be consulted, it will inspect the request, note the model <code>fake-llm</code> in the request, which will match the above InferenceModel, resulting in the request being routed to the <code>fake-llm-lora-a</code> model running as part of the cpu deployment we deployed earlier.</p> <p>Capture the gateway IP address to the environment variable <code>GW_IP</code>:</p> <pre><code>export GW_IP=$(kubectl get gtw inference-gateway -o jsonpath='{.status.addresses[0].value}')\n</code></pre> <p>Send a request:</p> <pre><code>curl $GW_IP/v1/completions -H 'Content-Type: application/json' -d '{\n  \"model\": \"fake-llm\",\n  \"prompt\": \"Write as if you were a critic: San Francisco\",\n  \"max_tokens\": 100,\n  \"temperature\": 0\n}' | jq\n</code></pre> <p>Here is a sample response:</p> <pre><code>{\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"This is fake LLM with LoRA-A. Greetings from module A!\"\n    }\n  ],\n  \"created\": 1742678062,\n  \"id\": \"fake-123\",\n  \"model\": \"fake-llm-lora-a\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 10,\n    \"prompt_tokens\": 10,\n    \"total_tokens\": 20\n  }\n}\n</code></pre> <p>Consider playing with the InferenceModel:</p> <ul> <li>Target the <code>fake-llm-lora-b</code> model instead.</li> <li>Make another <code>curl</code> request</li> <li>Check the response body and cnofirm that the request was handled by that model</li> </ul> <p>You can also set a 50/50 weight distribution and call both models.</p>"},{"location":"kgateway/","title":"Using the Inference extension with kgateway","text":"<p>This guide is similar to the GW API inference extension guide that uses Envoy Gateway. This guide focuses on the same scenario, but running with kgateway instead.</p> <p>The idea is to setup in a Kubernetes cluster a mechanism to route traffic to backend LLMs.</p>"},{"location":"kgateway/#prerequisites","title":"Prerequisites","text":"<ul> <li>amd64 CPU architecture (required by vLLM).</li> <li>Lots of CPU and RAM.</li> <li>Tools:  kubectl, helm</li> </ul>"},{"location":"kgateway/#a-k8s-cluster","title":"A k8s cluster","text":"<p>I chose k3d:</p> <pre><code>k3d cluster create my-k8s-cluster \\\n    --api-port 6443 \\\n    --k3s-arg \"--disable=traefik@server:0\" \\\n    --port 80:80@loadbalancer \\\n    --port 443:443@loadbalancer\n</code></pre>"},{"location":"kgateway/#install-kgateway","title":"Install kgateway","text":"<p>Install the GW API CRDs:</p> <pre><code>kubectl apply -f \\\n  https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.1/experimental-install.yaml\n</code></pre> <p>The inference extension CRDs also need to be installed:</p> <pre><code>k apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v0.2.0/manifests.yaml\n</code></pre> <p>Install the kgateway CRDs:</p> <pre><code>helm upgrade --install kgateway-crds \\\n  oci://cr.kgateway.dev/kgateway-dev/charts/kgateway-crds \\\n  --version v2.0.0-main \\\n  --namespace kgateway-system --create-namespace\n</code></pre> <p>Install kgateway:</p> <pre><code>helm upgrade --install kgateway \\\n  oci://cr.kgateway.dev/kgateway-dev/charts/kgateway \\\n  --version v2.0.0-main \\\n  --namespace kgateway-system --create-namespace \\\n  --set inferenceExtension.enabled=true\n</code></pre> <p>Above, note that the inference extension is enabled.</p>"},{"location":"kgateway/#deploy-an-llm-worload","title":"Deploy an LLM worload","text":"<p>The guide offers two options.  We opt for the cpu deployment option.</p> kgateway/cpu-deployment.yaml<pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-qwen-1-5b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-qwen-1-5b\n  template:\n    metadata:\n      labels:\n        app: vllm-qwen-1-5b\n    spec:\n      containers:\n        - name: lora\n          image: \"public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.0\" # formal images can be found in https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n          - \"--model\"\n          - \"Qwen/Qwen2.5-1.5B-Instruct\"\n          - \"--port\"\n          - \"8000\"\n          - \"--enable-lora\"\n          - \"--max-loras\"\n          - \"4\"\n          - \"--lora-modules\"\n          - '{\"name\": \"tweet-summary-0\", \"path\": \"SriSanth2345/Qwen-1.5B-Tweet-Generations\", \"base_model_name\": \"Qwen/Qwen2.5-1.5B\"}'\n          - '{\"name\": \"tweet-summary-1\", \"path\": \"SriSanth2345/Qwen-1.5B-Tweet-Generations\", \"base_model_name\": \"Qwen/Qwen2.5-1.5B\"}'\n          env:\n            - name: PORT\n              value: \"8000\"\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n            - name: VLLM_CPU_KVCACHE_SPACE\n              value: \"4\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 240\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 600\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          resources:\n             limits:\n               cpu: \"12\"\n               memory: \"9000Mi\"\n             requests:\n               cpu: \"12\"\n               memory: \"9000Mi\"\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts: # DO NOT USE subPath, dynamic configmap updates don't work on subPaths\n          - name: config-volume\n            mountPath:  /config\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-qwen-adapters\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-qwen-adapters\ndata:\n  configmap.yaml: |\n      vLLMLoRAConfig:\n        name: vllm-qwen-1-5b\n        port: 8000\n        ensureExist:\n          models:\n          - base-model: Qwen/Qwen2.5-1.5B\n            id: tweet-summary-1\n            source: SriSanth2345/Qwen-1.5B-Tweet-Generations\n</code></pre> <p>The cpu deployment option uses a relatively small model:  <code>Qwen/Qwen2.5-1.5B-Instruct</code>. This model is not gated and so does not require a Hugging Face token.</p> <p>The container requires 12 cpus and 9Gi of memory (see the <code>resources</code> section).</p> <p>The perhaps important thing to note about the running container is that it supposedly configures two models, <code>tweet-summary-0</code> and <code>tweet-summary-1</code> through lora adapters that extend the base model.</p> <p>The default deployment sets 3 replicas which requires a lot of resources. So I set my manifest to use just one replica.</p> <pre><code>k apply -f kgateway/cpu-deployment.yaml\n</code></pre> <p>Allow 5-10 minutes before the pod is up and running.</p>"},{"location":"kgateway/#the-inferencepool","title":"The InferencePool","text":"kgateway/inference-pool.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  name: qwen-pool\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-qwen-1-5b\n  extensionRef:\n    name: qwen-pool-endpoint-picker\n</code></pre> <p>The name of the pool is set to <code>qwen-pool</code>.</p> <p>The InferencePool resource appears to be concerned primarily with more practical aspects of how to route to the LLM workload:</p> <ol> <li>What port does it run (<code>targetPortNumber</code>), and </li> <li>How do I select or identify the workload (<code>selector</code>)</li> </ol> <pre><code>k apply -f kgateway/inference-pool.yaml\n</code></pre>"},{"location":"kgateway/#the-inferencemodel","title":"The InferenceModel","text":"kgateway/inference-model.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: qwen-inferencemodel\nspec:\n  modelName: tweet-summary\n  criticality: Critical\n  poolRef:\n    name: qwen-pool\n  targetModels:\n  - name: tweet-summary-1\n    weight: 100\n</code></pre> <p>The model achieves the following:</p> <ol> <li>Any requests specifying the model \"tweet-summary\" will match this inference model</li> <li>This model is marked with a <code>criticality</code> of Critical</li> <li>This model is associated with the above inference pool, meaning that matching requests will be routed to our cpu deployment</li> <li>Through <code>targetModels</code> we are configuring which of the two extension models (\"tweet-summary-0\" or \"tweet-summary-1\") to target, or what weight distributions to give to each.</li> </ol> <pre><code>k apply -f kgateway/inference-model.yaml\n</code></pre>"},{"location":"kgateway/#provision-a-gateway","title":"Provision a Gateway","text":"kgateway/gateway.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: inference-gateway\nspec:\n  gatewayClassName: kgateway\n  listeners:\n  - name: http\n    protocol: HTTP\n    port: 8080\n  - name: llm-gw\n    protocol: HTTP\n    port: 8081\n</code></pre> <p>The configuration is straightforward: the <code>gatewayClassName</code> is set to <code>kgateway</code>.</p> <p>It's curious that two listeners are configured, presumably to demonstrate that you can segregate LLM requests from non-LLM requests.  In other words, dedicate a listener for LLM requests named <code>llm-gw</code>.</p> <p>The inference extension is already hooked up to the gateway.</p> <pre><code>k apply -f kgateway/gateway.yaml\n</code></pre>"},{"location":"kgateway/#configure-the-route","title":"Configure the route","text":"kgateway/route.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - name: inference-gateway\n    sectionName: llm-gw\n  rules:\n  - backendRefs:\n    - group: inference.networking.x-k8s.io\n      kind: InferencePool\n      name: qwen-pool\n      port: 8000\n</code></pre> <p>The route forwards requests to the inference pool directly by referencing it as a <code>backendRef</code>.</p> <pre><code>k apply -f kgateway/route.yaml\n</code></pre>"},{"location":"kgateway/#test-it","title":"Test it","text":"<p>Everything is now in place:  requests can be sent in to the gateway, the inference extension will be consulted, it will inspect the request, note the model <code>tweet-summary</code> in the request, which will match the above InferenceModel, resulting in the request being routed to the <code>tweet-summary-1</code> model running as part of the cpu deployment we deployed earlier.</p> <p>Capture the gateway IP address to the environment variable <code>GW_IP</code>:</p> <pre><code>export GW_IP=$(kubectl get gtw inference-gateway -o jsonpath='{.status.addresses[0].value}')\n</code></pre> <p>Send a request:</p> <pre><code>curl $GW_IP:8081/v1/completions -H 'Content-Type: application/json' -d '{\n  \"model\": \"tweet-summary\",\n  \"prompt\": \"Write as if you were a critic: San Francisco\",\n  \"max_tokens\": 100,\n  \"temperature\": 0\n}' | jq\n</code></pre> <p>Here is a sample response:</p> <pre><code>{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"prompt_logprobs\": null,\n      \"stop_reason\": null,\n      \"text\": \" Giants - 2019\\n\\nThe San Francisco Giants have been one of the most successful teams in Major League Baseball over the past few years, and they continue to be a force to be reckoned with. The team has won three World Series championships in the last five seasons, including their first title since 1954.\\n\\nIn 2019, the Giants continued their dominance by winning the National League West divisional title for the third time in four years. They finished the\"\n    }\n  ],\n  \"created\": 1742577733,\n  \"id\": \"cmpl-30ea8f02-2aa7-4648-bb28-efe88c43517c\",\n  \"model\": \"tweet-summary-1\",\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 100,\n    \"prompt_tokens\": 10,\n    \"prompt_tokens_details\": null,\n    \"total_tokens\": 110\n  }\n}\n</code></pre> <p>Consider playing with the InferenceModel:</p> <ul> <li>Target the other model instead.</li> <li>Make another curl request</li> <li>Verify in the response that the request was handled by that model</li> </ul>"},{"location":"kgateway/#thoughts","title":"Thoughts","text":"<p>This is much better than the previous guide. The extension is still there but no longer needs to be managed explicitly.</p> <p>As I review this model diagram:</p> <p></p> <p>The configuration matches this picture perfectly:  the route uses the InferencePool as a <code>backendRef</code>, which in turn acts as a proxy for the backing LLM workload.</p> <p>This redresses the issues I cited from the previous guide.</p>"}]}