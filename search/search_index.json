{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Understanding the Inference Extension Guide","text":"<p>The GW API inference extension has [a guide] (https://gateway-api-inference-extension.sigs.k8s.io/guides) demonstrating how the extension works.</p> <p>The idea is to setup in a Kubernetes cluster a mechanism to route traffic to backend LLMs.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>amd64 CPU architecture (required by vLLM).</li> <li>Lots of CPU and RAM.</li> <li>Tools:  kubectl, helm</li> </ul>"},{"location":"#a-k8s-cluster","title":"A k8s cluster","text":"<p>I chose k3d:</p> <pre><code>k3d cluster create my-k8s-cluster \\\n    --api-port 6443 \\\n    --k3s-arg \"--disable=traefik@server:0\" \\\n    --port 80:80@loadbalancer \\\n    --port 443:443@loadbalancer\n</code></pre>"},{"location":"#envoy-gateway","title":"Envoy Gateway","text":"<p>The guide uses Envoy Gateway.</p> <pre><code>helm install eg oci://docker.io/envoyproxy/gateway-helm \\\n  --version v1.3.1 \\\n  -n envoy-gateway-system --create-namespace\n</code></pre> <p>It's interesting to point out that Envoy Gateway will automatically install the Kubernetes GW API CRDs as part of its installation.  So no need to do that explicitly.</p> <pre><code>k api-resources --api-group=gateway.networking.k8s.io\n</code></pre>"},{"location":"#enable-patch-policy","title":"Enable Patch Policy","text":"<p>This step is mostly artificial, or an artifact of Envoy Gateway conventions:  by default Envoy Gateway does not permit the application of policies that configure a gateway directly.</p> <p>All we are doing is telling Envoy Gateway through configuration:  enable it.</p> enable-patch-policy.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: envoy-gateway-config\n  namespace: envoy-gateway-system\ndata:\n# This manifest's main purpose is to set `enabledEnvoyPatchPolicy` to `true`.\n# This only needs to be ran once on your cluster (unless you'd like to change anything. i.e. enabling the admin dash)\n# Any field under `admin` is optional, and only for enabling the admin endpoints, for debugging.\n# Admin Interface: https://www.envoyproxy.io/docs/envoy/latest/operations/admin\n# PatchPolicy docs: https://gateway.envoyproxy.io/docs/tasks/extensibility/envoy-patch-policy/#enable-envoypatchpolicy \n  envoy-gateway.yaml: |\n    apiVersion: gateway.envoyproxy.io/v1alpha1\n    kind: EnvoyGateway\n    provider:\n      type: Kubernetes\n    gateway:\n      controllerName: gateway.envoyproxy.io/gatewayclass-controller\n    extensionApis:\n      enableEnvoyPatchPolicy: true      \n      enableBackend: true\n#    admin:\n#      enablePprof: true\n#      address:\n#        host: 127.0.0.1\n#        port: 19000\n#      enabledDumpConfig: true\n</code></pre> <pre><code>k apply -f enable-patch-policy.yaml\n</code></pre> <p>After the ConfigMap is revised, the deployment must be restarted to pick up the change:</p> <pre><code>k rollout restart deployment envoy-gateway -n envoy-gateway-system\n</code></pre>"},{"location":"#gateway-class","title":"Gateway Class","text":"<p>The installation of Envoy Gateway does not automatically beget a GatewayClass.</p> <p>So let's go ahead and create it:</p> gateway-class.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: eg\nspec:\n  controllerName: gateway.envoyproxy.io/gatewayclass-controller\n</code></pre> <pre><code>k apply -f gateway-class.yaml\n</code></pre>"},{"location":"#provision-a-gateway","title":"Provision a Gateway","text":"gateway.yaml<pre><code>---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: inference-gateway\nspec:\n  gatewayClassName: eg\n  listeners:\n    - name: http\n      protocol: HTTP\n      port: 8080\n    - name: llm-gw\n      protocol: HTTP\n      port: 8081\n</code></pre> <p>The configuration is straightforward: the gatewayClassName points to the eg gateway class.</p> <p>It's curious that two listeners are configured, presumably to demonstrate that you can segregate LLM requests from non-LLM requests.  In other words, dedicate a listener for LLM requests named <code>llm-gw</code>.</p> <pre><code>k apply -f gateway.yaml\n</code></pre>"},{"location":"#deploy-an-llm-worload","title":"Deploy an LLM worload","text":"<p>The guide offers two options.  We opt for the cpu deployment option.</p> cpu-deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama2-7b\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-llama2-7b\n  template:\n    metadata:\n      labels:\n        app: vllm-llama2-7b\n    spec:\n      containers:\n        - name: lora\n          image: \"public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.8.0\" # formal images can be found in https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo\n          imagePullPolicy: Always\n          command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n          args:\n          - \"--model\"\n          - \"Qwen/Qwen2.5-1.5B-Instruct\"\n          - \"--port\"\n          - \"8000\"\n          - \"--enable-lora\"\n          - \"--max-loras\"\n          - \"4\"\n          - \"--lora-modules\"\n          - '{\"name\": \"tweet-summary-0\", \"path\": \"SriSanth2345/Qwen-1.5B-Tweet-Generations\", \"base_model_name\": \"Qwen/Qwen2.5-1.5B\"}'\n          - '{\"name\": \"tweet-summary-1\", \"path\": \"SriSanth2345/Qwen-1.5B-Tweet-Generations\", \"base_model_name\": \"Qwen/Qwen2.5-1.5B\"}'\n          env:\n            - name: PORT\n              value: \"8000\"\n            - name: VLLM_ALLOW_RUNTIME_LORA_UPDATING\n              value: \"true\"\n            - name: VLLM_CPU_KVCACHE_SPACE\n              value: \"4\"\n          ports:\n            - containerPort: 8000\n              name: http\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 240\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 600\n            httpGet:\n              path: /health\n              port: http\n              scheme: HTTP\n            initialDelaySeconds: 5\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          resources:\n             limits:\n               cpu: \"12\"\n               memory: \"9000Mi\"\n             requests:\n               cpu: \"12\"\n               memory: \"9000Mi\"\n          volumeMounts:\n            - mountPath: /data\n              name: data\n            - mountPath: /dev/shm\n              name: shm\n            - name: adapters\n              mountPath: \"/adapters\"\n      initContainers:\n        - name: lora-adapter-syncer\n          tty: true\n          stdin: true\n          image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer:main\n          restartPolicy: Always\n          imagePullPolicy: Always\n          env:\n            - name: DYNAMIC_LORA_ROLLOUT_CONFIG\n              value: \"/config/configmap.yaml\"\n          volumeMounts: # DO NOT USE subPath, dynamic configmap updates don't work on subPaths\n          - name: config-volume\n            mountPath:  /config\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 30\n      volumes:\n        - name: data\n          emptyDir: {}\n        - name: shm\n          emptyDir:\n            medium: Memory\n        - name: adapters\n          emptyDir: {}\n        - name: config-volume\n          configMap:\n            name: vllm-qwen-adapters\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vllm-qwen-adapters\ndata:\n  configmap.yaml: |\n      vLLMLoRAConfig:\n        name: vllm-llama2-7b\n        port: 8000\n        ensureExist:\n          models:\n          - base-model: Qwen/Qwen2.5-1.5B\n            id: tweet-summary-1\n            source: SriSanth2345/Qwen-1.5B-Tweet-Generations\n</code></pre> <p>The cpu deployment option uses a relatively small model:  <code>Qwen/Qwen2.5-1.5B-Instruct</code>. This model is not gated and so does not require a Hugging Face token.</p> <p>The container requires 12 cpus and 9Gi of memory (see the <code>resources</code> section).</p> <p>The perhaps important thing to note about the running container is that it supposedly configures two models, <code>tweet-summary-0</code> and <code>tweet-summary-0</code> through lora adapters that extend the base model.</p> <p>The default deployment sets 3 replicas which requires a lot of resources. So I set my manifest to use just one replica.</p> <pre><code>k apply -f cpu-deployment.yaml\n</code></pre> <p>Allow 5-10 minutes before the pod is up and running.</p>"},{"location":"#inference-extension-crds","title":"Inference Extension CRDs","text":"<p>The inference extension provides two new resources: InferencePool and InferenceModel for configuring different facets of routing to LLM backends.</p> <p>The cluster needs to know about these resources:</p> <pre><code>k apply -f crds/\n</code></pre>"},{"location":"#install-the-extension","title":"Install the extension","text":"inference-extension.yaml<pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-llama2-7b-epp\n  namespace: default\nspec:\n  selector:\n    app: vllm-llama2-7b-epp\n  ports:\n    - protocol: TCP\n      port: 9002\n      targetPort: 9002\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vllm-llama2-7b-epp\n  namespace: default\n  labels:\n    app: vllm-llama2-7b-epp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: vllm-llama2-7b-epp\n  template:\n    metadata:\n      labels:\n        app: vllm-llama2-7b-epp\n    spec:\n      containers:\n      - name: epp\n        image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:main\n        imagePullPolicy: Always\n        args:\n        - -poolName\n        - \"vllm-llama2-7b\"\n        - -v\n        - \"4\"\n        - -grpcPort\n        - \"9002\"\n        - -grpcHealthPort\n        - \"9003\"\n        env:\n        - name: USE_STREAMING\n          value: \"true\"\n        ports:\n        - containerPort: 9002\n        - containerPort: 9003\n        - name: metrics\n          containerPort: 9090\n        livenessProbe:\n          grpc:\n            port: 9003\n            service: inference-extension\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          grpc:\n            port: 9003\n            service: inference-extension\n          initialDelaySeconds: 5\n          periodSeconds: 10\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: pod-read\nrules:\n- apiGroups: [\"inference.networking.x-k8s.io\"]\n  resources: [\"inferencemodels\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"inference.networking.x-k8s.io\"]\n  resources: [\"inferencepools\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"discovery.k8s.io\"]\n  resources: [\"endpointslices\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n--- \nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: pod-read-binding\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: default\nroleRef:\n  kind: ClusterRole\n  name: pod-read\n</code></pre> <pre><code>k apply -f inference-extension.yaml\n</code></pre>"},{"location":"#hook-in-the-extension","title":"Hook in the extension","text":"<p>To hook the extension into the flow of requests through the gateway's <code>llm-gw</code> listener on port 8081, we define a ficticious backend and a route to it:</p> backend-and-route.yaml<pre><code>---\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: Backend\nmetadata:\n  name: backend-dummy\nspec:\n  endpoints:\n    - fqdn:\n        # Both these values are arbitrary and unused as the PatchPolicy redirects requests.\n        hostname: 'foo.bar.com'\n        port: 8080\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: llm-route\nspec:\n  parentRefs:\n  - name: inference-gateway\n    sectionName: llm-gw\n  rules:\n  - backendRefs:\n    - group: gateway.envoyproxy.io\n      kind: Backend\n      name: backend-dummy\n    timeouts:\n      request: \"24h\"\n      backendRequest: \"24h\"\n</code></pre> <p>Note that the route has no match clause, meaning any request to that listener matches.</p> <pre><code>k apply -f backend-and-route.yaml\n</code></pre> <p>Next, we define an EG extension policy:</p> extension-policy.yaml<pre><code>---\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyExtensionPolicy\nmetadata:\n  name: ext-proc-policy\n  namespace: default\nspec:\n  extProc:\n    - backendRefs:\n      - group: \"\"\n        kind: Service\n        name: vllm-llama2-7b-epp\n        port: 9002\n      processingMode:\n        allowModeOverride: true\n        request:\n          body: Buffered\n        response:\n      # The timeouts are likely not needed here. We can experiment with removing/tuning them slowly.\n      # The connection limits are more important and will cause the opaque: ext_proc_gRPC_error_14 error in Envoy GW if not configured correctly. \n      messageTimeout: 1000s\n      backendSettings:\n        circuitBreaker:\n          maxConnections: 40000\n          maxPendingRequests: 40000\n          maxParallelRequests: 40000\n        timeout:\n          tcp:\n            connectTimeout: 24h\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    name: llm-route\n</code></pre> <p>The extension policy achieves two things:</p> <ol> <li>It makes EG aware of the inference extension you just deployed through the <code>extProc</code> section, and</li> <li>It attaches the extension to the above route</li> </ol> <pre><code>k apply -f extension-policy.yaml\n</code></pre>"},{"location":"#envoy-patch-policy","title":"Envoy Patch Policy","text":"<p>The guide also applies an Envoy patch policy.</p> patch-policy.yaml<pre><code>---\napiVersion: gateway.envoyproxy.io/v1alpha1\nkind: EnvoyPatchPolicy\nmetadata:\n  name: custom-response-patch-policy\n  namespace: default\nspec:\n  targetRef:\n    group: gateway.networking.k8s.io\n    kind: Gateway\n    name: inference-gateway\n  type: JSONPatch\n  jsonPatches:\n    # Necessary to create a cluster of the type: ORIGINAL_DST to allow for \n    # direct pod scheduling. Which is heavily utilized in our scheduling.\n    # Specifically the field `original_dst_lb_config` allows us to enable\n    # `use_http_header` and `http_header_name`. \n    # Source: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/cluster/v3/cluster.proto\n    - type: \"type.googleapis.com/envoy.config.cluster.v3.Cluster\"\n      name: original_destination_cluster\n      operation:\n        op: add\n        path: \"\"\n        value:\n          name: original_destination_cluster\n          type: ORIGINAL_DST\n          original_dst_lb_config:\n            use_http_header: true\n            http_header_name: \"x-gateway-destination-endpoint\"\n          connect_timeout: 1000s\n          lb_policy: CLUSTER_PROVIDED\n          dns_lookup_family: V4_ONLY\n          circuit_breakers:\n            thresholds:\n            - max_connections: 40000\n              max_pending_requests: 40000\n              max_requests: 40000\n\n    # This ensures that envoy accepts untrusted certificates. We tried to explicitly\n    # set TrustChainVerification to ACCEPT_UNSTRUSTED, but that actually didn't work\n    # and what worked is setting the common_tls_context to empty.\n    - type: \"type.googleapis.com/envoy.config.cluster.v3.Cluster\"\n      name: \"envoyextensionpolicy/default/ext-proc-policy/extproc/0\"\n      operation:\n        op: add\n        path: \"/transport_socket\"\n        value:\n          name: \"envoy.transport_sockets.tls\"\n          typed_config:\n            \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\"\n            common_tls_context: {}\n    - type: \"type.googleapis.com/envoy.config.route.v3.RouteConfiguration\"\n      name: default/inference-gateway/llm-gw\n      operation:\n        op: replace\n        path: \"/virtual_hosts/0/routes/0/route/cluster\"\n        value: original_destination_cluster\n# Comment the below to disable full duplex streaming\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: add\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/request_body_mode\"\n        value: FULL_DUPLEX_STREAMED\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: add\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/request_trailer_mode\"\n        value: SEND\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: add\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/response_body_mode\"\n        value: FULL_DUPLEX_STREAMED\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: replace\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/response_trailer_mode\"\n        value: SEND\n    - type: \"type.googleapis.com/envoy.config.listener.v3.Listener\"\n      name: \"default/inference-gateway/llm-gw\"\n      operation:\n        op: replace\n        path: \"/default_filter_chain/filters/0/typed_config/http_filters/0/typed_config/processing_mode/response_header_mode\"\n        value: SEND\n</code></pre> <p>I don't really understand what this policy does or its purpose, if it circumvents a bug, or what.</p> <pre><code>k apply -f patch-policy.yaml\n</code></pre>"},{"location":"#configure-the-extension","title":"Configure the extension","text":"<p>Now that the extension is in the path of requests, we need to give it the information it needs to do its job:  it needs to know about the backend LLMs, associate model names to each, and other metadata such as \"criticality\" in order to know the relative importance of the different models.</p>"},{"location":"#the-inferencepool","title":"The InferencePool","text":"inference-pool.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferencePool\nmetadata:\n  labels:\n  name: vllm-llama2-7b\nspec:\n  targetPortNumber: 8000\n  selector:\n    app: vllm-llama2-7b\n  extensionRef:\n    name: vllm-llama2-7b-epp\n</code></pre> <p>Thename of the pool is set to <code>vllm-llama2-7b</code>.</p> <p>The InferencePool resource appears to be concerned primarily with more practical aspects of how to route to the LLM workload:</p> <ol> <li>What port does it run (<code>targetPortNumber</code>), and </li> <li>How do I select or identify the workload (<code>selector</code>)</li> </ol> <pre><code>k apply -f inference-pool.yaml\n</code></pre>"},{"location":"#the-inferencemodel","title":"The InferenceModel","text":"inference-model.yaml<pre><code>---\napiVersion: inference.networking.x-k8s.io/v1alpha2\nkind: InferenceModel\nmetadata:\n  name: inferencemodel-sample\nspec:\n  modelName: tweet-summary\n  criticality: Critical\n  poolRef:\n    name: vllm-llama2-7b\n  targetModels:\n  - name: tweet-summary-1\n    weight: 100\n</code></pre> <p>The model achieves the following:</p> <ol> <li>Any requests specifying the model \"tweet-summary\" will match this inference model</li> <li>This model is marked with a <code>criticality</code> of Critical</li> <li>This model is associated with the above inference pool, meaning that matching requests will be routed to our cpu deployment</li> <li>Through <code>targetModels</code> we are configuring which of the two extension models (\"tweet-summary-0\" or \"tweet-summary-1\") to target, or what weight distributions to give to each.</li> </ol> <pre><code>k apply -f inference-model.yaml\n</code></pre>"},{"location":"#test-it","title":"Test it","text":"<p>Everything is now in place:  requests can be sent in to the gateway, the inference extension will be consulted, it will inspect the request, note the model <code>tweet-summary</code> in the request, which will match the above InferenceModel, resulting in the request being routed to the <code>tweet-summary-1</code> model running as part of the cpu deployment we deployed earlier.</p> <p>I suspect that the name \"llama2-7b\" is an artifact of perhaps a prior attempt to make this exercise work with a different model.  The model was revised to Qwen-1.5B but the name of the deployment and associated labels were never updated to match?</p> <p>Capture the gateway ip address to the environment variable <code>GW_IP</code>:</p> <pre><code>export GW_IP=$(kubectl get gtw inference-gateway -o jsonpath='{.status.addresses[0].value}')\n</code></pre> <p>Send a request:</p> <pre><code>curl $GW_IP:8081/v1/completions -H 'Content-Type: application/json' -d '{\n  \"model\": \"tweet-summary\",\n  \"prompt\": \"Write as if you were a critic: San Francisco\",\n  \"max_tokens\": 100,\n  \"temperature\": 0\n}' | jq\n</code></pre> <p>Consider playing with the InferenceModel:</p> <ul> <li>Target the other model instead.</li> <li>Make another curl request</li> <li>Verify in the response that the request was handled by that model</li> </ul>"},{"location":"#thoughts","title":"Thoughts","text":"<p>As I review this model diagram:</p> <p></p> <p>The configuration doesn't match this picture perfectly.</p> <p>Clearly the route and inference pool are associated to one another through the extension, which is not depicted above.</p> <p>In my opinion the extension ought to be transparent, and the inference resources attached directly to the route.</p> <p>Also the dichotomy of InferencePool and InferenceModel don't make very much sense to me, yet, perhaps due to lack of familiarity with this project.</p>"}]}